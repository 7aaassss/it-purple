{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install clickhouse-connect sentence_transformers transformers clickhouse_driver spacy\n!python -m spacy download ru_core_news_sm\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#делаем все необходимые импорты\nimport numpy as np\nimport torch\n\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\nfrom transformers import set_seed\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport clickhouse_connect\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM,AutoModelForCausalLM,AutoModelForSequenceClassification\nfrom sentence_transformers import SentenceTransformer, util\nfrom utils.generators import SpellChecker, ToxicityClassifier\n\nimport clickhouse_connect\n\nfrom scipy.spatial.distance import cdist\nimport spacy\nimport re\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nset_seed(42)\n\nnltk.download('punkt')\ntorch.cuda.is_available()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Загрузка модели для эмбеддинга\nembedding_model = SentenceTransformer(\"intfloat/multilingual-e5-large\")\n\n# Загрузка токенизатора и модели T5\nt5_tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/FRED-T5-1.7B\")\nt5_model = AutoModelForSeq2SeqLM.from_pretrained(\"ai-forever/FRED-T5-1.7B\")\n\n#Загрузим проверщика орфографии \nspelchecker_model_name = 'UrukHan/t5-russian-spell'\n\nspell_checker_tokenizer = T5TokenizerFast.from_pretrained(spelchecker_model_name)\nspell_checker_model = AutoModelForSeq2SeqLM.from_pretrained(spelchecker_model_name).to(device).eval()\n\n#Проверка на токсичность вопросов\ntoxicity_detection_model_name = 'cointegrated/rubert-tiny-toxicity'\ntoxicity_detection_tokenizer = AutoTokenizer.from_pretrained(toxicity_detection_model_name)\ntoxicity_detection_model = AutoModelForSequenceClassification.from_pretrained(toxicity_detection_model_name).to(device).eval()","metadata":{"execution":{"iopub.status.busy":"2024-03-17T11:52:45.118656Z","iopub.execute_input":"2024-03-17T11:52:45.119199Z","iopub.status.idle":"2024-03-17T11:52:45.451963Z","shell.execute_reply.started":"2024-03-17T11:52:45.119166Z","shell.execute_reply":"2024-03-17T11:52:45.450734Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Загрузка модели для эмбеддинга\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintfloat/multilingual-e5-large\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Загрузка токенизатора и модели T5\u001b[39;00m\n\u001b[1;32m      5\u001b[0m t5_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mai-forever/FRED-T5-1.7B\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'SentenceTransformer' is not defined"],"ename":"NameError","evalue":"name 'SentenceTransformer' is not defined","output_type":"error"}]},{"cell_type":"code","source":"#подключаемся к нашей баззе данных\nclient = clickhouse_connect.get_client(\n    host='x2ar8i584r.europe-west4.gcp.clickhouse.cloud',\n    port=8443,\n    username='default',\n    password='Avk7dO5kAfK_s'  # Замените на ваш реальный пароль\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#получаем и предобрабатываем данные из базы данных\nresult = client.query('SELECT text,url from \"document\"')\ndocs_with_links = [\n#     {'text': 'Текст документа 1', 'link': 'https://ссылка_на_документ_1'},\n#     {'text': 'Текст документа 2', 'link': 'https://ссылка_на_документ_2'},\n]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#заполняем наш список текстами\nfor row in result.result_rows:\n    # Удаляем специальные символы, кроме букв, цифр, пробелов, точек и запятых, заменяем серии пробелов на одинарные\n    redacted_text = re.sub(r'[^\\w\\s\\.,]+', '', row[0])  # Удаляем нежелательные символы\n    redacted_text = re.sub(r'\\s+', ' ', redacted_text)  # Заменяем серии пробелов на одинарные пробелы\n    redacted_text = redacted_text.replace('\\n', ' ').replace('\\r', ' ').strip()  # Удаляем переносы строк и возвраты каретки, если они есть\n    \n    docs_with_links.append({\n        \"text\": redacted_text,\n        \"url\": row[1]\n    })\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(docs_with_links)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ниже закоментирован код деления данных на чанки(батчи), части текстов, которые в будущем и будут являтся контекстом для генерации модели. Закуоментирован он, т.к он выполняет запаолнение базы данных и не нужен нам каждый раз","metadata":{}},{"cell_type":"code","source":"# #описиываем  деление текстов на чанки(подтексты)\n# nlp = spacy.load(\"ru_core_news_sm\")\n# def create_chunks_by_sentences(text, sentences_per_chunk=2):\n#     doc = nlp(text)\n#     chunks = []\n#     current_chunk = []\n#     1\n#     for sent in doc.sents:\n#         current_chunk.append(sent.text.strip())\n        \n#         # Если в текущем чанке накопилось 2 предложения, сохраняем его и начинаем новый\n#         if len(current_chunk) == sentences_per_chunk:\n#             chunks.append(\" \".join(current_chunk))\n#             current_chunk = []\n    \n#     # Добавляем последний чанк, если он содержит предложения\n#     if current_chunk:\n#         chunks.append(\" \".join(current_chunk))\n        \n#     return chunks\n\n# def get_chunk_embeddings(data):\n#     return embedding_model.encode(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# chunk_id = 0  # Начальный ID для чанков\n\n# for doc in docs_with_links[:3]:\n#     text_chunks = create_chunks_by_sentences(doc['text'])\n    \n#     for chunk_text in text_chunks:\n#         # Получаем эмбеддинг для чанка текста\n#         embedding = get_chunk_embeddings(chunk_text)  # Это должен быть список float чисел\n        \n#         # Преобразуем список в строку в формате ClickHouse Array\n#         embedding_str = '[' + ','.join(map(str, embedding)) + ']'\n        \n#         # Экранирование кавычек в тексте для SQL запроса\n#         chunk_text_escaped = chunk_text.replace(\"'\", \"''\")\n        \n#         # Формируем SQL запрос для вставки текста и эмбеддинга как массива\n#         sql = f\"INSERT INTO text_embeddings (chunk_id, chunk_text, embedding, source_url) VALUES ({chunk_id}, '{chunk_text_escaped}', {embedding_str}, '{doc['url']}')\"\n        \n#         # Выполнение запроса\n#         try:\n#             client.query(sql)\n#         except Exception as e:\n#             print(f\"Ошибка при вставке данных: {e}\")\n        \n#         chunk_id += 1\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ниже представлена реализация RAG(Retrieval Augmented Generation). Сначала мы находим n наиболее релевантных к нашему запросов чанков, объединяем их в 1 контекст и передаём модели для генерации ответа на его основе","metadata":{}},{"cell_type":"code","source":"#метод извлекающий чанки и их эмбединги из баззы данных\ndef fetch_embeddings_from_db():\n    query_result = client.query('SELECT chunk_text, embedding,source_url FROM text_embeddings')\n    chunks = []\n    embeddings = []\n    urls = []\n    for row in query_result.result_rows:\n        chunk_text, embedding,url = row  # предполагаем, что embedding уже в нужном формате\n        chunks.append(chunk_text)\n        urls.append(url)\n        # Преобразуем список в numpy массив, если он уже не в таком формате\n        embedding_array = np.array(embedding, dtype=np.float32)\n        \n        # Добавляем эмбеддинг в список embeddings\n        embeddings.append(embedding_array)\n    \n    \n    return chunks, urls, np.array(embeddings)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#получаем чанки и их эмбединги, а так же ссылки на документы, откуда чанк был взят\nchunks, urls, chunk_embeddings = fetch_embeddings_from_db()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#имеем 15 тысяч чанков и 15 тысячей эмбедингов\nlen(chunks),chunk_embeddings.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RETRIEVING + CONETXT GETTING","metadata":{}},{"cell_type":"code","source":"spell_checker = SpellChecker(spell_checker_tokenizer, spell_checker_model)\ntoxicity_detector = ToxicityClassifier(toxicity_detection_tokenizer, toxicity_detection_model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def retrieve_top_k_documents(query, top_k):\n    query_embedding = embedding_model.encode([query])[0]\n    cos_similarities = cosine_similarity(query_embedding.reshape(1, -1), np.array(chunk_embeddings))\n    sorted_indices = np.argsort(cos_similarities[0])[::-1]\n\n    unique_chunks = set()\n    top_chunks = []\n    top_indices = []\n    for idx in sorted_indices:\n        if len(top_chunks) >= top_k:\n            break\n        chunk = chunks[idx]\n        if chunk not in unique_chunks:\n            unique_chunks.add(chunk)\n            top_chunks.append(chunk)\n            top_indices.append(idx)\n\n    return top_chunks, top_indices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_context_from_query(query):\n    top_chunks, top_k_indices = retrieve_top_k_documents(query, top_k=4)\n    return \"\\n\\n\".join([f\"{chunk}\\nИсточник: {urls[top_k_indices[i]]}\" for i, chunk in enumerate(top_chunks)])\n    \n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Применяем RAG и скармливаем полученный и обработанный контекст модели для генерации","metadata":{}},{"cell_type":"code","source":"def get_response(prompt):\n    input_ids = t5_tokenizer(prompt, return_tensors=\"pt\").input_ids\n    outputs = t5_model.generate(\n    input_ids, \n    max_length=512, \n    num_beams=5, \n    early_stopping=True,\n    do_sample=True, \n    top_k=50, \n    top_p=0.95, \n    repetition_penalty=2.5\n)\n\n    answer = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return answer\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Задаёте интерсующий Вас вопрос\nquery = input()\nif toxicity_detector(query):\n    print(\"Грубый вопрос\")\ncorrected_query = spell_checker(query)\n#для вашего вопроса автоматически формируется контекст\ncontext = get_context_from_query(corrected_query)\n#контекст + вопрос формируют промт для модели\nprompt = f\"\"\"\nQuestion: {corrected_query}\n\nContext : {context}\n\n\"\"\"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ответ модели:","metadata":{}},{"cell_type":"code","source":"print(get_response(prompt))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}